{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Enriched Master Dataset\n",
    "\n",
    "This notebook reads the cleaned yearly CSV files and adds supplemental columns needed for analysis.\n",
    "\n",
    "**What this does:**\n",
    "- Reads all `cleaned_*.csv` files from `../data/cleaned/`\n",
    "- Adds supplemental columns (SEVIS_ID, IS_STEM, geographic mappings, etc.)\n",
    "- Outputs a single enriched master Parquet file\n",
    "\n",
    "**Supporting data required:**\n",
    "- DHS STEM CIP code list (2024)\n",
    "- CIP code to NSF subject field mapping\n",
    "- ZIP code to LMA mapping (quarterly, 2010-2024)\n",
    "- ZIP code to county mapping (quarterly, 2010-2024)\n",
    "- Working population by county (yearly, 2004-2023)\n",
    "\n",
    "**Run this once** after cleaning the data and before creating staging tables.\n",
    "\n",
    "**Expected runtime:** 10-30 minutes depending on your machine and data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to DuckDB with optimized settings for M3 Mac (18GB RAM)\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Connect to temporary DuckDB database (will persist for this notebook session)\n",
    "con = duckdb.connect('tmp_pipeline.db')\n",
    "\n",
    "# Configure DuckDB for large datasets (optimized for M3 Mac with 18GB RAM)\n",
    "con.execute(\"SET memory_limit='14GB'\")  # Leave ~4GB for OS and other processes\n",
    "con.execute(\"SET threads=6\")  # M3 can handle more threads efficiently\n",
    "con.execute(\"SET preserve_insertion_order=false\")  # Disable to save memory\n",
    "\n",
    "print(\"Connected to DuckDB with optimized settings for M3 Mac (18GB RAM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Input: ../data/cleaned/cleaned_*_all.csv\n",
      "  Output: ../data/sevis_f1_enriched_master.parquet\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "RAW_DATA_PATH = '../data/cleaned/cleaned_*_all.csv'\n",
    "SUPPORTING_DATA_DIR = '../data/supporting'\n",
    "OUTPUT_PATH = '../data/sevis_f1_enriched_master.parquet'\n",
    "\n",
    "# Supporting data files\n",
    "DHS_STEM_LIST = f'{SUPPORTING_DATA_DIR}/dhs_stem_cip_code_list_July2024.csv'\n",
    "CIP_TO_NSF_MAPPING = f'{SUPPORTING_DATA_DIR}/cip_code_to_nsf_subject_field_mapping.csv'\n",
    "ZIP_LMA_MAPPING = f'{SUPPORTING_DATA_DIR}/zip_county_lma_quarterly.csv'\n",
    "ZIP_COUNTY_CROSSWALK = f'{SUPPORTING_DATA_DIR}/HUD_zip_code_to_county_crosswalk_2010-2024.csv'\n",
    "WORKING_POP_BY_COUNTY = f'{SUPPORTING_DATA_DIR}/working_pop_by_county_fips_2004-2023.csv'\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Input: {RAW_DATA_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Base Table with SEVIS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base data from CSV files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5be8a8a3b743e5a233e01aebc9e55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 70,230,345 rows\n",
      "✓ Added SEVIS_ID column (Year + Individual_Key)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base data from CSV files...\")\n",
    "\n",
    "# Override all date columns and CIP code columns as VARCHAR to handle invalid/unparseable values\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE base_data AS\n",
    "    SELECT\n",
    "      *,\n",
    "      CONCAT(CAST(Year AS VARCHAR), Individual_Key) AS SEVIS_ID\n",
    "    FROM read_csv_auto('../data/cleaned/cleaned_*_all.csv',\n",
    "                       union_by_name=true,\n",
    "                       types={\n",
    "                         'FIRST_ENTRY_DATE': 'VARCHAR',\n",
    "                         'LAST_ENTRY_DATE': 'VARCHAR',\n",
    "                         'LAST_DEPARTURE_DATE': 'VARCHAR',\n",
    "                         'VISA_ISSUE_DATE': 'VARCHAR',\n",
    "                         'VISA_EXPIRATION_DATE': 'VARCHAR',\n",
    "                         'PROGRAM_START_DATE': 'VARCHAR',\n",
    "                         'PROGRAM_END_DATE': 'VARCHAR',\n",
    "                         'AUTHORIZATION_START_DATE': 'VARCHAR',\n",
    "                         'AUTHORIZATION_END_DATE': 'VARCHAR',\n",
    "                         'OPT_AUTHORIZATION_START_DATE': 'VARCHAR',\n",
    "                         'OPT_AUTHORIZATION_END_DATE': 'VARCHAR',\n",
    "                         'OPT_EMPLOYER_START_DATE': 'VARCHAR',\n",
    "                         'OPT_EMPLOYER_END_DATE': 'VARCHAR',\n",
    "                         'Major_1_CIP_Code': 'VARCHAR',\n",
    "                         'Major_2_CIP_Code': 'VARCHAR',\n",
    "                         'Minor_CIP_Code': 'VARCHAR'\n",
    "                       })\n",
    "\"\"\")\n",
    "\n",
    "# Display row count\n",
    "result = con.execute(\"SELECT COUNT(*) as total_rows FROM base_data\").df()\n",
    "print(f\"✓ Loaded {result['total_rows'][0]:,} rows\")\n",
    "print(f\"✓ Added SEVIS_ID column (Year + Individual_Key)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add IS_STEM Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding IS_STEM column...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96dbc6782fa4903b530c2cdee1aa684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryException",
     "evalue": "Out of Memory Error: failed to offload data block of size 96.0 KiB (74.7 GiB/74.7 GiB used).\nThis limit was set by the 'max_temp_directory_size' setting.\nBy default, this setting utilizes the available disk space on the drive where the 'temp_directory' is located.\nYou can adjust this setting, by using (for example) PRAGMA max_temp_directory_size='10GiB'\n\nPossible solutions:\n* Reducing the number of threads (SET threads=X)\n* Disabling insertion-order preservation (SET preserve_insertion_order=false)\n* Increasing the memory limit (SET memory_limit='...GB')\n\nSee also https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the DHS STEM list (override type to VARCHAR since CIP codes are categorical identifiers)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    CREATE OR REPLACE TEMP TABLE dhs_stem_list AS\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    SELECT * FROM read_csv_auto(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDHS_STEM_LIST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m                                types=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020_cip_code\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVARCHAR\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    CREATE OR REPLACE TEMP TABLE with_stem AS\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m    SELECT\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m      b.*,\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m      CASE\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m        -- If all CIP fields are missing/blank, keep unknown as NULL\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m        WHEN NULLIF(b.Major_1_CIP_Code, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) IS NULL\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m         AND NULLIF(b.Major_2_CIP_Code, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) IS NULL\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m         AND NULLIF(b.Minor_CIP_Code, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) IS NULL\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m        THEN NULL\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m        -- Otherwise TRUE if any CIP matches the DHS STEM list\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m        ELSE EXISTS (\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m          SELECT 1\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m          FROM (\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m            SELECT UNNEST([b.Major_1_CIP_Code, b.Major_2_CIP_Code, b.Minor_CIP_Code]) AS cip\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m          ) cips\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43m          JOIN dhs_stem_list AS stem_list\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43m            ON LOWER(stem_list.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2020_cip_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) = LOWER(cips.cip)\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m        )\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m      END AS IS_STEM\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43m    FROM base_data b\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Display STEM statistics\u001b[39;00m\n\u001b[1;32m     34\u001b[0m result \u001b[38;5;241m=\u001b[39m con\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m    SELECT\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m      COUNT(*) as total,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m    FROM with_stem\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdf()\n",
      "\u001b[0;31mOutOfMemoryException\u001b[0m: Out of Memory Error: failed to offload data block of size 96.0 KiB (74.7 GiB/74.7 GiB used).\nThis limit was set by the 'max_temp_directory_size' setting.\nBy default, this setting utilizes the available disk space on the drive where the 'temp_directory' is located.\nYou can adjust this setting, by using (for example) PRAGMA max_temp_directory_size='10GiB'\n\nPossible solutions:\n* Reducing the number of threads (SET threads=X)\n* Disabling insertion-order preservation (SET preserve_insertion_order=false)\n* Increasing the memory limit (SET memory_limit='...GB')\n\nSee also https://duckdb.org/docs/stable/guides/performance/how_to_tune_workloads"
     ]
    }
   ],
   "source": [
    "print(\"Adding IS_STEM column...\")\n",
    "\n",
    "# Load the DHS STEM list (override type to VARCHAR since CIP codes are categorical identifiers)\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE dhs_stem_list AS\n",
    "    SELECT * FROM read_csv_auto('{DHS_STEM_LIST}',\n",
    "                                types={{'2020_cip_code': 'VARCHAR'}})\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE with_stem AS\n",
    "    SELECT\n",
    "      b.*,\n",
    "      CASE\n",
    "        -- If all CIP fields are missing/blank, keep unknown as NULL\n",
    "        WHEN NULLIF(b.Major_1_CIP_Code, '') IS NULL\n",
    "         AND NULLIF(b.Major_2_CIP_Code, '') IS NULL\n",
    "         AND NULLIF(b.Minor_CIP_Code, '') IS NULL\n",
    "        THEN NULL\n",
    "        -- Otherwise TRUE if any CIP matches the DHS STEM list\n",
    "        ELSE EXISTS (\n",
    "          SELECT 1\n",
    "          FROM (\n",
    "            SELECT UNNEST([b.Major_1_CIP_Code, b.Major_2_CIP_Code, b.Minor_CIP_Code]) AS cip\n",
    "          ) cips\n",
    "          JOIN dhs_stem_list AS stem_list\n",
    "            ON LOWER(stem_list.\"2020_cip_code\") = LOWER(cips.cip)\n",
    "        )\n",
    "      END AS IS_STEM\n",
    "    FROM base_data b\n",
    "\"\"\")\n",
    "\n",
    "# Display STEM statistics\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) as total,\n",
    "      SUM(CASE WHEN IS_STEM = TRUE THEN 1 ELSE 0 END) as stem_count,\n",
    "      SUM(CASE WHEN IS_STEM IS NULL THEN 1 ELSE 0 END) as unknown_count\n",
    "    FROM with_stem\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ IS_STEM column added\")\n",
    "print(f\"  STEM students: {result['stem_count'][0]:,}\")\n",
    "print(f\"  Unknown: {result['unknown_count'][0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Add NSF_SUBJ_FIELD_BROAD Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding NSF subject field mappings...\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE with_nsf AS\n",
    "    SELECT\n",
    "      w.*,\n",
    "      m.NSF_BROAD_FIELD AS NSF_SUBJ_FIELD_BROAD\n",
    "    FROM with_stem w\n",
    "    LEFT JOIN (\n",
    "      SELECT DISTINCT\n",
    "        -- Normalize CIP code to MM.mmmm format\n",
    "        CONCAT(\n",
    "          LPAD(REGEXP_EXTRACT(CIPCODE_ORIGINAL, '(\\\\d+)', 1), 2, '0'),\n",
    "          '.',\n",
    "          RPAD(COALESCE(REGEXP_EXTRACT(CIPCODE_ORIGINAL, '\\\\d+\\\\.(\\\\d+)$', 1), ''), 4, '0')\n",
    "        ) AS cip_normalized,\n",
    "        NSF_BROAD_FIELD\n",
    "      FROM read_csv_auto('{CIP_TO_NSF_MAPPING}')\n",
    "    ) m\n",
    "      ON CONCAT(\n",
    "           LPAD(REGEXP_EXTRACT(w.Major_1_CIP_Code, '(\\\\d+)', 1), 2, '0'),\n",
    "           '.',\n",
    "           RPAD(COALESCE(REGEXP_EXTRACT(w.Major_1_CIP_Code, '\\\\d+\\\\.(\\\\d+)$', 1), ''), 4, '0')\n",
    "         ) = m.cip_normalized\n",
    "\"\"\")\n",
    "\n",
    "# Display mapping statistics\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT\n",
    "      COUNT(*) as total,\n",
    "      COUNT(NSF_SUBJ_FIELD_BROAD) as mapped_count\n",
    "    FROM with_nsf\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ NSF subject field column added\")\n",
    "print(f\"  Mapped: {result['mapped_count'][0]:,} of {result['total'][0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add Geographic Columns (LMA and County)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating geographic lookup tables...\")\n",
    "\n",
    "# Create ZIP→LMA lookup\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE zip_lma_slim AS\n",
    "    WITH cleaned AS (\n",
    "      SELECT\n",
    "        CAST(YEAR AS INTEGER) AS year,\n",
    "        CAST(QUARTER AS INTEGER) AS quarter,\n",
    "        SUBSTR(REGEXP_REPLACE(TRIM(zip5), '[^0-9]', ''), 1, 5) AS zip5_norm,\n",
    "        lma_name\n",
    "      FROM read_csv_auto('{ZIP_LMA_MAPPING}')\n",
    "    )\n",
    "    SELECT year, quarter, zip5_norm, lma_name\n",
    "    FROM cleaned\n",
    "    WHERE year IS NOT NULL\n",
    "      AND quarter BETWEEN 1 AND 4\n",
    "      AND LENGTH(zip5_norm) = 5\n",
    "      AND zip5_norm <> '00000'\n",
    "    QUALIFY ROW_NUMBER() OVER (\n",
    "      PARTITION BY year, quarter, zip5_norm\n",
    "      ORDER BY lma_name\n",
    "    ) = 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ ZIP→LMA lookup created\")\n",
    "\n",
    "# Create ZIP→County lookup\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE zip_county_slim AS\n",
    "    WITH cw AS (\n",
    "      SELECT\n",
    "        CAST(YEAR AS INTEGER) AS year,\n",
    "        CAST(QUARTER AS INTEGER) AS quarter,\n",
    "        SUBSTR(REGEXP_REPLACE(TRIM(ZIP), '[^0-9]', ''), 1, 5) AS zip5_norm,\n",
    "        COUNTY AS county5\n",
    "      FROM read_csv_auto('{ZIP_COUNTY_CROSSWALK}')\n",
    "    ),\n",
    "    wp AS (\n",
    "      SELECT\n",
    "        LPAD(CAST(CAST(TRIM(CAST(County_LAUS_areacode AS VARCHAR)) AS INTEGER) AS VARCHAR), 5, '0') AS county5,\n",
    "        County_Name_State_Abbreviation\n",
    "      FROM read_csv_auto('{WORKING_POP_BY_COUNTY}')\n",
    "    )\n",
    "    SELECT\n",
    "      cw.year,\n",
    "      cw.quarter,\n",
    "      cw.zip5_norm,\n",
    "      wp.County_Name_State_Abbreviation\n",
    "    FROM cw\n",
    "    JOIN wp USING (county5)\n",
    "    WHERE cw.year IS NOT NULL\n",
    "      AND cw.quarter BETWEEN 1 AND 4\n",
    "      AND LENGTH(cw.zip5_norm) = 5\n",
    "      AND cw.zip5_norm <> '00000'\n",
    "    QUALIFY ROW_NUMBER() OVER (\n",
    "      PARTITION BY year, quarter, zip5_norm\n",
    "      ORDER BY County_Name_State_Abbreviation\n",
    "    ) = 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ ZIP→County lookup created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Joining geographic data...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE with_geo AS\n",
    "    SELECT \n",
    "      n.*,\n",
    "      lma_campus.lma_name AS CAMPUS_LMA,\n",
    "      lma_employer.lma_name AS EMPLOYER_LMA,\n",
    "      county_campus.County_Name_State_Abbreviation AS CAMPUS_COUNTY,\n",
    "      county_employer.County_Name_State_Abbreviation AS EMPLOYER_COUNTY\n",
    "    FROM with_nsf n\n",
    "    LEFT JOIN zip_lma_slim lma_campus\n",
    "      ON EXTRACT(YEAR FROM TRY_CAST(n.Program_End_Date AS DATE)) = lma_campus.year\n",
    "     AND EXTRACT(QUARTER FROM TRY_CAST(n.Program_End_Date AS DATE)) = lma_campus.quarter\n",
    "     AND n.Campus_Zip_Code = lma_campus.zip5_norm\n",
    "    LEFT JOIN zip_lma_slim lma_employer\n",
    "      ON EXTRACT(YEAR FROM TRY_CAST(n.Program_End_Date AS DATE)) = lma_employer.year\n",
    "     AND EXTRACT(QUARTER FROM TRY_CAST(n.Program_End_Date AS DATE)) = lma_employer.quarter\n",
    "     AND n.Employer_Zip_Code = lma_employer.zip5_norm\n",
    "    LEFT JOIN zip_county_slim county_campus\n",
    "      ON EXTRACT(YEAR FROM TRY_CAST(n.Program_End_Date AS DATE)) = county_campus.year\n",
    "     AND EXTRACT(QUARTER FROM TRY_CAST(n.Program_End_Date AS DATE)) = county_campus.quarter\n",
    "     AND n.Campus_Zip_Code = county_campus.zip5_norm\n",
    "    LEFT JOIN zip_county_slim county_employer\n",
    "      ON EXTRACT(YEAR FROM TRY_CAST(n.Program_End_Date AS DATE)) = county_employer.year\n",
    "     AND EXTRACT(QUARTER FROM TRY_CAST(n.Program_End_Date AS DATE)) = county_employer.quarter\n",
    "     AND n.Employer_Zip_Code = county_employer.zip5_norm\n",
    "\"\"\")\n",
    "\n",
    "# Display mapping statistics\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "      COUNT(*) as total,\n",
    "      COUNT(CAMPUS_LMA) as campus_lma_count,\n",
    "      COUNT(EMPLOYER_LMA) as employer_lma_count\n",
    "    FROM with_geo\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Geographic columns added\")\n",
    "print(f\"  Campus LMA mapped: {result['campus_lma_count'][0]:,}\")\n",
    "print(f\"  Employer LMA mapped: {result['employer_lma_count'][0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Working Population Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating working population lookup tables...\")\n",
    "\n",
    "# Unpivot working population data\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE workpop_long AS\n",
    "    SELECT\n",
    "      County_Name_State_Abbreviation,\n",
    "      state_name,\n",
    "      CAST(year_col AS INTEGER) AS year,\n",
    "      CASE\n",
    "        WHEN LOWER(TRIM(pop_value)) IN ('missing data', '') THEN NULL\n",
    "        ELSE CAST(REPLACE(TRIM(pop_value), ',', '') AS INTEGER)\n",
    "      END AS working_pop\n",
    "    FROM (\n",
    "      SELECT\n",
    "        County_Name_State_Abbreviation,\n",
    "        state_name,\n",
    "        UNNEST([\n",
    "          '2004','2005','2006','2007','2008','2009','2010','2011','2012','2013',\n",
    "          '2014','2015','2016','2017','2018','2019','2020','2021','2022','2023','2024'\n",
    "        ]) AS year_col,\n",
    "        UNNEST([\n",
    "          LMA_WORKING_POP_2004, LMA_WORKING_POP_2005, LMA_WORKING_POP_2006, LMA_WORKING_POP_2007,\n",
    "          LMA_WORKING_POP_2008, LMA_WORKING_POP_2009, LMA_WORKING_POP_2010, LMA_WORKING_POP_2011,\n",
    "          LMA_WORKING_POP_2012, LMA_WORKING_POP_2013, LMA_WORKING_POP_2014, LMA_WORKING_POP_2015,\n",
    "          LMA_WORKING_POP_2016, LMA_WORKING_POP_2017, LMA_WORKING_POP_2018, LMA_WORKING_POP_2019,\n",
    "          LMA_WORKING_POP_2020, LMA_WORKING_POP_2021, LMA_WORKING_POP_2022, LMA_WORKING_POP_2023,\n",
    "          LMA_WORKING_POP_2024\n",
    "        ]) AS pop_value\n",
    "      FROM read_csv_auto('{WORKING_POP_BY_COUNTY}')\n",
    "    )\n",
    "    WHERE year BETWEEN 2004 AND 2024\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Working population by county unpivoted\")\n",
    "\n",
    "# Create state-level aggregates\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE workpop_state AS\n",
    "    SELECT\n",
    "      state_name,\n",
    "      year,\n",
    "      SUM(working_pop) AS state_working_pop\n",
    "    FROM workpop_long\n",
    "    WHERE state_name IS NOT NULL\n",
    "    GROUP BY state_name, year\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ State-level working population aggregated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding working population columns...\")\n",
    "\n",
    "# Create final enriched master table\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMP TABLE enriched_master AS\n",
    "    SELECT \n",
    "      g.*,\n",
    "      wp_lma.working_pop AS EMPLOYER_LMA_WORKPOP_YR,\n",
    "      wp_state.state_working_pop AS EMPLOYER_STATE_WORKPOP_YR\n",
    "    FROM with_geo g\n",
    "    LEFT JOIN workpop_long wp_lma\n",
    "      ON g.EMPLOYER_COUNTY = wp_lma.County_Name_State_Abbreviation\n",
    "     AND EXTRACT(YEAR FROM TRY_CAST(g.Authorization_Start_Date AS DATE)) = wp_lma.year\n",
    "    LEFT JOIN workpop_state wp_state\n",
    "      ON g.Employer_State = wp_state.state_name\n",
    "     AND EXTRACT(YEAR FROM TRY_CAST(g.Authorization_Start_Date AS DATE)) = wp_state.year\n",
    "\"\"\")\n",
    "\n",
    "# Display final statistics\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "      COUNT(*) as total,\n",
    "      COUNT(EMPLOYER_LMA_WORKPOP_YR) as lma_workpop_count,\n",
    "      COUNT(EMPLOYER_STATE_WORKPOP_YR) as state_workpop_count\n",
    "    FROM enriched_master\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Working population columns added\")\n",
    "print(f\"  LMA working pop mapped: {result['lma_workpop_count'][0]:,}\")\n",
    "print(f\"  State working pop mapped: {result['state_workpop_count'][0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Enriched Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting to Parquet...\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# Export to Parquet with compression\n",
    "con.execute(f\"\"\"\n",
    "    COPY (\n",
    "      SELECT * FROM enriched_master\n",
    "    ) TO '{OUTPUT_PATH}' (FORMAT PARQUET, COMPRESSION ZSTD, ROW_GROUP_SIZE 100000)\n",
    "\"\"\")\n",
    "\n",
    "# Display final file size\n",
    "file_size = os.path.getsize(OUTPUT_PATH)\n",
    "print(f\"\\n✓ Enriched master dataset created!\")\n",
    "print(f\"  Output: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size / (1024**3):.2f} GB\")\n",
    "print(f\"\\n✓ Next step: Run create_staging_tables.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up: close connection and remove temporary database\n",
    "con.close()\n",
    "\n",
    "if os.path.exists('tmp_pipeline.db'):\n",
    "    os.remove('tmp_pipeline.db')\n",
    "    print(\"✓ Temporary database cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OPT Observatory)",
   "language": "python",
   "name": "opt-observatory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
