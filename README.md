# OPT Observatory

Analysis pipeline for F-1 student OPT (Optional Practical Training) participation patterns using SEVIS data.

## ğŸ“Š Overview

This repository provides a reproducible analysis pipeline for studying F-1 international student transitions to Optional Practical Training (OPT) employment in the United States. The analysis uses SEVIS (Student and Exchange Visitor Information System) data obtained through FOIA requests.

**Key Features:**
- DuckDB-based data pipeline (no database server required)
- Reproducible analysis with exact package versions (renv)
- Pre-cleaned data ready for analysis
- Geographic enrichment (Labor Market Areas, counties)
- Field of study classification (NSF subject fields, STEM designation)

## ğŸš€ Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/OPT-observatory.git
cd OPT-observatory
```

### 2. Download Data

Download the data bundle from Zenodo:
- **DOI:** [Will be added after upload]
- **Size:** ~50 GB (compressed)
- **Contents:**
  - Raw SEVIS F-1 data (2004-2023)
  - Pre-cleaned SEVIS data (2004-2023)
  - Supporting data files (HUD, BLS LAUS, DHS, NSF)

Extract the data:
```bash
# Download data.zip from Zenodo link above
unzip data.zip
# This creates the data/ folder with raw/, cleaned/, and supporting/ subdirectories
```

### 3. Install R Dependencies

```bash
./setup.sh
```

This installs all required R packages using renv (reproducible package management).

### 4. Run Analysis Pipeline

Open and run the notebooks in order:

```bash
jupyter notebook
```

1. **`notebooks/create_enriched_master.ipynb`**
   - Reads cleaned SEVIS data
   - Adds supplemental columns (STEM status, NSF fields, geographic mappings)
   - Outputs enriched Parquet file

2. **`notebooks/create_staging_tables.ipynb`**
   - Creates analysis-ready staging tables
   - Outputs Parquet and CSV files

3. **Analysis notebooks** (e.g., `grad_cohort_opt.ipynb`, `geographic_retention.ipynb`)
   - Run specific analyses on staging tables

## ğŸ“ Repository Structure

```
OPT-observatory/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ setup.sh                      # Install R dependencies
â”œâ”€â”€ renv.lock                     # R package versions (reproducibility)
â”œâ”€â”€ .gitignore                    # Excludes large data files
â”œâ”€â”€ data/                         # Downloaded from Zenodo (not in Git)
â”‚   â”œâ”€â”€ raw/                      # Raw SEVIS FOIA files (20 CSV files, ~25 GB)
â”‚   â”œâ”€â”€ cleaned/                  # Pre-cleaned files (20 CSV files, ~24 GB)
â”‚   â”œâ”€â”€ supporting/               # Supporting datasets (~500 MB)
â”‚   â”‚   â”œâ”€â”€ dhs_stem_cip_code_list_July2024.csv
â”‚   â”‚   â”œâ”€â”€ cip_code_to_nsf_subject_field_mapping.csv
â”‚   â”‚   â”œâ”€â”€ working_pop_by_county_fips_2004-2023.csv
â”‚   â”‚   â”œâ”€â”€ HUD_zip_code_to_county_crosswalk_2010-2024.csv
â”‚   â”‚   â””â”€â”€ zip_county_lma_quarterly.csv
â”‚   â””â”€â”€ staging/                  # Generated by pipeline
â”œâ”€â”€ notebooks/                    # Analysis notebooks
â”‚   â”œâ”€â”€ create_enriched_master.ipynb
â”‚   â”œâ”€â”€ create_staging_tables.ipynb
â”‚   â””â”€â”€ ...                       # Analysis notebooks
â””â”€â”€ scripts/
    â””â”€â”€ load_data_parallel.R      # Data cleaning script (for transparency)
```

## ğŸ”§ Prerequisites

### Required Software

- **R 4.5+** - Statistical computing
  - macOS: `brew install r`
  - Linux: `sudo apt-get install r-base`
  - Windows: [Download from CRAN](https://cran.r-project.org/)

- **Jupyter** - For running notebooks
  ```bash
  pip install jupyter
  ```

- **Git** - Version control
  - macOS: Pre-installed or `brew install git`
  - Linux: `sudo apt-get install git`
  - Windows: [Download from git-scm.com](https://git-scm.com/)

### R Packages

All R packages are managed by **renv** and installed automatically by `setup.sh`:
- duckdb - SQL database engine
- tidyverse - Data manipulation
- data.table - Fast data processing
- lubridate - Date handling
- fs - File system operations
- future/future.apply - Parallel processing

## ğŸ“– Data

### SEVIS F-1 Data

FOIA request to U.S. Department of Homeland Security / Student and Exchange Visitor Program (SEVP)
- **Coverage:** 2004-2023
- **Records:** ~XX million F-1 student records
- **Includes:** Demographics, program information, OPT employment details

### Supporting Data

1. **DHS STEM CIP Code List** (July 2024)
   - Official list of STEM-designated degree programs
   - Source: Department of Homeland Security

2. **CIP to NSF Field Mapping**
   - Maps CIP codes to NSF broad/major/fine subject fields
   - Generated from NSF 7-field taxonomy

3. **HUD ZIP-County Crosswalk** (2010-2024)
   - Quarterly ZIP code to county FIPS mappings
   - Source: U.S. Department of Housing and Urban Development

4. **BLS LAUS Working Population** (2004-2023)
   - Annual civilian working population by county
   - Labor Market Area (LMA) designations
   - Source: Bureau of Labor Statistics Local Area Unemployment Statistics

5. **ZIP-LMA Mapping** (Quarterly, 2010-2024)
   - Joins HUD crosswalk with LMA data
   - Time-aware mapping for accurate geographic attribution

## ğŸ”¬ Data Pipeline

### Stage 1: Enrichment (`create_enriched_master.ipynb`)

**Input:** Cleaned SEVIS CSV files (`data/cleaned/*.csv`)

**Processing:**
1. Create unique SEVIS_ID (Year + Individual_Key)
2. Add IS_STEM flag (matches DHS STEM CIP list)
3. Add NSF_SUBJ_FIELD_BROAD (maps CIP â†’ NSF taxonomy)
4. Add geographic columns:
   - CAMPUS_LMA / EMPLOYER_LMA (Labor Market Areas)
   - CAMPUS_COUNTY / EMPLOYER_COUNTY
5. Add working population columns for normalization

**Output:** `data/sevis_f1_enriched_master.parquet` (~2-5 GB)

**Runtime:** 10-30 minutes

### Stage 2: Staging Tables (`create_staging_tables.ipynb`)

**Input:** Enriched master Parquet file

**Processing:**
- Creates analysis-specific subsets
- Optimizes for query performance
- Exports as Parquet and CSV

**Output:** `data/staging/*.parquet` and `*.csv`

**Runtime:** 5-15 minutes

### Stage 3: Analysis

Run analysis notebooks to generate insights, visualizations, and statistics.

## ğŸ”„ Reproducing the Data Cleaning (Optional)

The data bundle includes both **raw** and **cleaned** files for transparency. If you want to verify the cleaning process:

1. Review the cleaning script: `scripts/load_data_parallel.R`
2. Run it on the raw files:
   ```bash
   Rscript scripts/load_data_parallel.R
   ```
3. Compare output to provided cleaned files

The cleaning process:
- Standardizes column names
- Parses and validates dates
- Handles missing values
- Removes duplicate records
- Filters to F-1 visa records

## ğŸ“ Citation

If you use this data or code, please cite:

```
[Author names]. (2025). OPT Observatory: Analysis Pipeline for F-1 OPT Participation.
Zenodo. https://doi.org/10.5281/zenodo.XXXXX
```

And cite the original SEVIS data source:
```
U.S. Department of Homeland Security, Student and Exchange Visitor Program (SEVP).
SEVIS F-1 Student Data (2004-2023). Obtained via FOIA request [case number].
```

## ğŸ“„ License

[Specify your license - e.g., MIT, CC BY 4.0, etc.]

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

For questions or issues:
- **GitHub Issues:** [Link to issues]
- **Email:** [Your contact email]

## âš ï¸ Notes

- **Data Size:** The full dataset is ~50 GB. Ensure you have sufficient disk space.
- **Runtime:** Initial pipeline run takes 15-45 minutes depending on your machine.
- **Memory:** Minimum 8 GB RAM recommended; 16 GB+ preferred for large analyses.
- **Privacy:** The data has been de-identified. No individual-level personally identifiable information (PII) is included.

## ğŸ™ Acknowledgments

Data sources:
- U.S. Department of Homeland Security / SEVP
- U.S. Department of Housing and Urban Development
- Bureau of Labor Statistics
- National Science Foundation

Tools:
- [DuckDB](https://duckdb.org/) - In-process SQL database
- [renv](https://rstudio.github.io/renv/) - R package management
- [Tidyverse](https://www.tidyverse.org/) - R data science packages
